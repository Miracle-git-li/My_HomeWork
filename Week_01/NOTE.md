# 决策树知识点
# 常见决策树

| 模型      | ID3      |     C4.5 |   CART   |
| :--------| :-------- | --------:| :------: |
| 结构   | 多叉树    |   多叉树 |  二叉树  |
| 特征选择   | 信息增益    |   信息增益率 |  Gini系数/均方差  |
| 连续值处理   | 不支持    |   支持 |  支持  |
| 缺失值处理   | 不支持    |   支持 |  支持  |
| 枝剪   | 不支持    |   支持 |  支持  |


# 简述决策树构建过程
1. 构建根节点，将所有训练数据都放在根节点
2. 选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类
3. 如果子集非空，或子集容量未小于最少数量，递归1，2步骤，直到所有训练数据子集都被正确分类或没有合适的特征为止

# 详述信息熵计算方法及存在问题
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g925k1ns0yj305y018glg.jpg)

其中，D为数据全集，C为不同因变量类别k上的子集(传统意义上的y的种类)

# 详述信息增益计算方法
条件信息熵：在特征A给定的条件下对数据集D分类的不确定性：
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g925puuv1cj308p01kdfr.jpg)

信息增益：知道特征A的信息而使类D的信息的不确定减少的程度（对称）：
I(D,A) = H(D)-H(D/A)

简而言之，就是在特征A下找到最合适的切分，使得在该切分下信息量的变换最大，更加稳定；但是这个有一个问题，对于类别天生较多的特征，模型更容易选中，因为特征类别较多，切分后的信息增益天生更大，更容易满足我们的原始假设

# 详述信息增益率计算方法
在信息增益计算的基础不变的情况下得到的：I(D,A) = H(D)-H(D/A)，同时还考虑了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g925yr9z1vj306601f745.jpg),用划分的子集数上的熵来平衡了分类数过多的问题。

信息增益率：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9260xx5wej304b017q2r.jpg)

# 解释Gini系数
Gini系数二分情况下：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g92705elt9j308000q744.jpg)

对于决策树样本D来说，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9273px3t2j30a4018wee.jpg)

对于样本D，如果根据特征A的某个值，把D分成D1和D2，则在特征A的条件下，D的基尼系数为：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9274yiwhoj30iq02wmx8.jpg)

# ID3存在的问题
缺点：
- 存在偏向于选择取值较多的特征问题
- 连续值不支持
- 缺失值不支持
- 无法枝剪


# C4.5相对于ID3的改进点
- 主动进行的连续的特征离散化
    - 比如m个样本的连续特征A有m个，先set在order，再两两组合求中间值，以该值点作为划分的待选点
    - **连续特征可以再后序特征划分中仍可继续参与计算**
- 缺失问题优化
    - 训练：用所有未缺失的样本，和之前一样，计算每个属性的信息增益，但是这里的信息增益需要乘以一个系数（未缺失样本/总样本）
    - 预测：直接跳过该节点，并将此样本划入所有子节点，划分后乘以系数计算，系数为不缺失部分的样本分布
- 采用预枝剪

# CART的连续特征改进点
- 分类情况下的变量特征选择
    - 离散变量：二分划分
    - 连续变量：和C4.5一致，如果当前节点为连续属性，则该属性后面依旧可以参与子节点的产生选择过程
- 回归情况下，连续变量不再采取中间值划分，采用最小方差法

# CART分类树建立算法的具体流程
我们的算法从根节点开始，用训练集递归的建立CART树。
- 对于当前节点的数据集为D，如果样本个数小于阈值或者没有特征，则返回决策子树，当前节点停止递归
- 计算样本集D的基尼系数，如果基尼系数小于阈值，则返回决策树子树，当前节点停止递归
- 计算当前节点现有的各个特征的各个特征值对数据集D的基尼系数
- 在计算出来的各个特征的各个特征值对数据集D的基尼系数中，选择基尼系数最小的特征A和对应的特征值a。根据这个最优特征和最优特征值，把数据集划分成两部分D1和D2，同时建立当前节点的左右节点，做节点的数据集D为D1，右节点的数据集D为D2
- 递归1～4

# CART回归树建立算法的具体流程
其他部分都一样，在构建过程中遇到连续值的话，并不是利用C4.5中的中间值基尼系数的方式，而是采取了最小方差方法：

对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点，其中c1为D1的均值，c2为D2的均值：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g928jep1nkj309a00q745.jpg)

# CART输出结果的逻辑？
- 回归树：利用最终叶子的均值或者中位数来作为输出结果
- 分类树：利用最终叶子的大概率的分类类别来作为输出结果

# CART树算法的剪枝过程是怎么样的？
目标函数为：𝐶𝛼(𝑇𝑡)=𝐶(𝑇𝑡)+𝛼|𝑇𝑡|，其中，α为正则化参数，这和线性回归的正则化一样。𝐶(𝑇𝑡)为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。|𝑇𝑡|是子树T的叶子节点的数量

当𝛼=0时，即没有正则化，原始的生成的CART树即为最优子树。当𝛼=∞时，即正则化强度达到最大，此时由原始的生成的CART树的根节点组成的单节点树为最优子树。
当然，这是两种极端情况。一般来说，𝛼越大，则剪枝剪的越厉害，生成的最优子树相比原生决策树就越偏小。对于固定的𝛼，一定存在使损失函数𝐶𝛼(𝑇)最小的唯一子树。
由枝剪到根结点及不枝剪两种情况可得：𝛼=(𝐶(𝑇)−𝐶(𝑇𝑡))/(|𝑇𝑡|−1) , C(T)为根结点误差

- 计算出每个子树是否剪枝的阈值𝛼
- 选择阈值𝛼集合中的最小值
- 分别针对不同的最小值𝛼所对应的剪枝后的最优子树做交叉验证

# 树形结构为何不需要归一化？
无论是分类树还是回归树，无论是连续变量还是离散变量，树模型一直想找的是最优切分点，不存在梯度导数等计算，数值缩放不影响分裂点位置，对树模型的结构不造成影响。

# 决策树的优缺点
优点：
- 缺失值不敏感，对特征的宽容程度高，可缺失可连续可离散
- 可解释性强
- 算法对数据没有强假设
- 可以解决线性及非线性问题
- 有特征选择等辅助功能

缺点：
- 处理关联性数据比较薄弱
- 正负量级有偏样本的样本效果较差
- 单棵树的拟合效果欠佳，容易过拟合  

# 随机森林知识点  

# 解释下随机森林?
- 随机森林=bagging+决策树
- 随机：特征选择随机+数据采样随机
    - 特征随机是在决策树**每个结点上选择的时候随机**，并不是在每棵树创建的时候随机
    - 每个结点上对特征选择都是从全量特征中进行采样对，**不会剔除已利用的**
    - 数据采样，是有放回的采样
        - 1个样本**未被选到**的概率为p = (1 - 1/N)^N = 1/e，即为OOB
- 森林：多决策树组合
    - 可分类可回归，回归是对输出值进行简单平均，分类是对输出值进行简单投票

# 随机森林用的是什么树？
CART树

# 随机森林的生成过程？
- 生成单棵决策树
    - 随机选取样本
    - 从M个输入特征里随机选择m个输入特征，然后从这m个输入特征里选择一个最好的进行分裂
    - 不需要剪枝，直到该节点的所有训练样例都属于同一类
- 生成若干个决策树

# 解释下随机森林节点的分裂策略？
Gini系数

在连续值和离散值上：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)

# 随机森林的损失函数是什么？
- 分类RF对应的CART分类树默认是基尼系数gini,另一个可选择的标准是信息增益
- 回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae
- 参考决策树的损失函数即可：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)

# 为了防止随机森林过拟合可以怎么做?
- 增加树的数量
- 增加叶子结点的数据数量
- bagging算法中，基模型的期望与整体期望一致，参考[就理论角度论证Bagging、Boosting的方差偏差问题](https://github.com/sladesha/Reflection_Summary/blob/master/基础概念/方差与偏差/方差与偏差.md#L7)
- 随着基模型数（m）的增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高

# 随机森林特征选择的过程？
特征选择方向：对于某个特征，如果用另外一个随机值替代它之后的表现比之前更差，则表明该特征比较重要，所占的权重应该较大，不能用一个随机值替代。相反，如果随机值替代后的表现没有太大差别，则表明该特征不那么重要，可有可无
    - 通过permutation的方式将原来的所有N个样本的第i个特征值重新打乱分布（相当于重新洗牌）
    - 是使用uniform或者gaussian抽取随机值替换原特征
    
# 是否用过随机森林，有什么技巧?
- 除了直接让随机森林选择特征，还有自行构造组合特征带入模型，是的randomForest-subspace变成randomForest-combination

# RF的参数有哪些，如何调参？
要调整的参数主要是 n_estimators和max_features 
- n_estimators是森林里树的数量，通常数量越大，效果越好，但是计算时间也会随之增加。 此外要注意，当树的数量超过一个临界值之后，算法的效果并不会很显著地变好
- max_features是分割节点时考虑的特征的随机子集的大小。 这个值越低，方差减小得越多，但是偏差的增大也越多
    - 回归：max_features = n_features
    - 分类：max_features = sqrt(n_features)

其他参数中
- class_weight也可以调整正负样本的权重
-  max_depth = None 和 min_samples_split = 2 结合，为不限制生成一个不修剪的完全树
 
# RF的优缺点 ？
- 优点:
    - 不同决策树可以由不同主机并行训练生成，效率很高
    - 随机森林算法继承了CART的优点
    - 将所有的决策树通过bagging的形式结合起来，避免了单个决策树造成过拟合的问题
- 缺点：
    - 没有严格数学理论支持  

# GBDT知识点  
# 介绍一下Boosting的思想？
- 初始化训练一个弱学习器，初始化下的各条样本的权重一致
- 根据上一个弱学习器的结果，调整权重，使得错分的样本的权重变得更高
- 基于调整后的样本及样本权重训练下一个弱学习器
- 预测时直接串联综合各学习器的加权结果

# 最小二乘回归树的切分过程是怎么样的？
- 回归树在每个切分后的结点上都会有一个预测值，这个预测值就是结点上所有值的均值
- 分枝时遍历所有的属性进行二叉划分，挑选使平方误差最小的划分属性作为本节点的划分属性
- 属性上有多个值，则需要遍历所有可能的属性值，挑选使平方误差最小的划分属性值作为本属性的划分值
- 递归重复以上步骤，直到满足叶子结点上值的要求

# 有哪些直接利用了Boosting思想的树模型？
adaboost，gbdt等等

# gbdt和boostingtree的boosting分别体现在哪里？
- boostingtree利用基模型学习器，拟合的是mse（回归）或者指数损失函数（分类）
- gbdt利用基模型学习器，拟合的是当前模型与标签值的损失函数的负梯度

# gbdt的中的tree是什么tree？有什么特征？
Cart tree，但是都是回归树

# 常用回归问题的损失函数？
- mse:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94bmopzfjj303s011t8i.jpg)
    - 负梯度：y-h(x)
    - 初始模型F0由目标变量的平均值给出
- 绝对损失:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94bne7cipj303400q742.jpg)
    - 负梯度：sign(y-h(x))
    - 初始模型F0由目标变量的中值给出
- Huber损失：mse和绝对损失的结合
    - 负梯度：y-h(x)和sign(y-h(x))分段函数
    - 它是MSE和绝对损失的组合形式，对于远离中心的异常点，采用绝对损失，其他的点采用MSE，这个界限一般用分位数点度量

# 常用分类问题的损失函数？
- 对数似然损失函数
    - 二元且标签y属于{-1,+1}：𝐿(𝑦,𝑓(𝑥))=𝑙𝑜𝑔(1+𝑒𝑥𝑝(−𝑦𝑓(𝑥)))
        - 负梯度：y/(1+𝑒𝑥𝑝(−𝑦𝑓(𝑥)))
    - 多元：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94cj1yce6j306x01iglg.jpg)
- 指数损失函数:𝐿(𝑦,𝑓(𝑥))=𝑒𝑥𝑝(−𝑦𝑓(𝑥))
    - 负梯度：y·𝑒𝑥𝑝(−𝑦𝑓(𝑥))
除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同

# 什么是gbdt中的损失函数的负梯度？
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94afa5h1lj3060017mx0.jpg)

当loss函数为均方误差![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ajgymkuj303w011jr6.jpg)，gbdt中的残差的负梯度的结果y-H(x)正好与boostingtree的拟合残差一致

# 如何用损失函数的负梯度实现gbdt？
- 利用![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94afa5h1lj3060017mx0.jpg)可以计算得到x对应的损失函数的负梯度![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ars964uj301h00ijr5.jpg),据此我们可以构造出第t棵回归树，其对应的叶子结点区域![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94atb0k7zj300p00i3y9.jpg)j为叶子结点位置
- 构建回归树的过程中，需要考虑找到特征A中最合适的切分点，使得切分后的数据集D1和D2的均方误差最小![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94b0klia2j30ee017aa0.jpg)
- 针对每一个叶子节点里的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的输出值𝑐𝑡𝑗,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94b5ffnyoj308g0173yd.jpg)
    - 首先，根据feature切分后的损失均方差大小，选取最优的特征切分
    - 其次，根据选定的feature切分后的叶子结点数据集，选取最使损失函数最小，也就是拟合叶子节点最好的输出值
    - 这样就完整的构造出一棵树：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94bfr5cn5j303d01kjr6.jpg)
- 本轮最终得到的强学习器的表达式如下：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94binx5prj307o01k0sl.jpg)

# 拟合损失函数的负梯度为什么是可行的？
- 泰勒展开的一阶形式：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94h6rkexqj305x00ijr7.jpg)
- m轮树模型可以写成：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94h8zovulj305v00iglf.jpg)
    - 对![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hak9e26j303n00idfm.jpg)进行泰勒展开：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hd5sz8vj309400kglg.jpg),其中m-1轮对残差梯度为![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hey2xksj3052017a9w.jpg)
- 我们拟合了残差的负梯度，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hi3epnaj302r00kmwx.jpg),所以![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hd5sz8vj309400kglg.jpg)内会让损失向下降对方向前进

# 即便拟合损失函数负梯度是可行的，为什么不直接拟合残差？ 拟合负梯度好在哪里？
- 前者不用残差的负梯度而是使用残差，是全局最优值，后者使用的是 局部最优方向（负梯度）*步长（𝛽）
- 依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此 当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，Boosting Tree也很难处理回归之外问题。 而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理

# Shrinkage收缩的作用？
每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易得到精确值，即它不完全信任每一棵残差树，认为每棵树只学到了真理的一部分累加的时候只累加了一小部分多学几棵树来弥补不足。 这个技巧类似于梯度下降里的学习率
- 原始：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94i9bokjzj306g00iglf.jpg)
- Shrinkage：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94iawlfq3j307600it8j.jpg)

# feature属性会被重复多次使用么？
会，同时因为特征会进行多次使用，特征用的越多，则该特征的重要性越大

# gbdt如何进行正则化的？
- 子采样
    - 每一棵树基于原始原本的一个子集进行训练
    - rf是有放回采样，gbdt是无放回采样
    - 特征子采样可以来控制模型整体的方差
- 利用Shrinkage收缩，控制每一棵子树的贡献度
- 每棵Cart树的枝剪

# 为什么集成算法大多使用树类模型作为基学习器？或者说，为什么集成学习可以在树类模型上取得成功？
- 对数据的要求比较低，不需要强假设，不需要数据预处理，连续离散都可以，缺失值也能接受
- bagging，关注于提升分类器的泛化能力
- boosting，关注于提升分类器的精度

# gbdt的优缺点？
优点：
- 数据要求比较低，不需要前提假设，能处理缺失值，连续值，离散值
- 使用一些健壮的损失函数，对异常值的鲁棒性非常强
- 调参相对较简单

缺点：
- 并行化能力差

# gbdt和randomforest区别？
- 相同：
    - 都是多棵树的组合
- 不同：
    - RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本
    - gbdt对异常值比rf更加敏感
    - gbdt是串行，rf是并行
    - gbdt是cart回归树，rf是cart分类回归树都可以
    - gbdt是提高降低偏差提高性能，rf是通过降低方差提高性能
    - gbdt对输出值是进行加权求和，rf对输出值是进行投票或者平均
    
# GBDT和LR的差异？
- 从决策边界来说，线性回归的决策边界是一条直线，逻辑回归的决策边界是一条曲线，而GBDT的决策边界可能是很多条线
- 当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。因为LR有参数惩罚，GBDT容易造成过拟合  

# xgboost知识点  

# xgboost对比gbdt/boosting Tree有了哪些方向上的优化？
- 显示的把树模型复杂度作为正则项加到优化目标中
- 优化目标计算中用到二阶泰勒展开代替一阶，更加准确
- 实现了分裂点寻找近似算法
    - 暴力枚举
    - 近似算法（分桶）
- 更加高效和快速
    - 数据事先排序并且以block形式存储，有利于并行计算
    - 基于分布式通信框架rabit，可以运行在MPI和yarn上
    - 实现做了面向体系结构的优化，针对cache和内存做了性能优化

# xgboost和gbdt的区别？
- 模型优化上：
    - 基模型的优化：
        - gbdt用的是cart回归树作为基模型，xgboost还可以用线性模型，加上天生的正则项，就是带L1和L2逻辑回归(分类)和线性回归(回归)
    - 损失函数上的优化：
        - gbdt对loss是泰勒一阶展开，xgboost是泰勒二阶展开
        - gbdt没有在loss中带入结点个数和预测值的正则项
    - 特征选择上的优化：
        - 实现了一种分裂节点寻找的近似算法，用于加速和减小内存消耗，而不是gbdt的暴力搜索
        - 节点分裂算法解决了缺失值方向的问题，gbdt则是沿用了cart的方法进行加权
    - 正则化的优化：
        - 特征采样
        - 样本采样
- 工程优化上：
    - xgboost在对特征进行了分block预排序，使得在做特征分裂的时候，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行
    - cache-aware, out-of-core computation
    - 支持分布式计算可以运行在MPI，YARN上，得益于底层支持容错的分布式通信框架rabit
    
# xgboost优化目标/损失函数改变成什么样？
- 原始：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mjezeisj307401fmx0.jpg)
    - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mnp3fd7j301700idfl.jpg)为泰勒一阶展开，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mrtqxv0j30480173yc.jpg)
- 改变：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mldvhz5j30ay01k3yf.jpg)
    - J为叶子结点的个数，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mo56g1vj300o00e0s1.jpg)为第j个叶子结点中的最优值    
    - ![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mnp3fd7j301700idfl.jpg)为泰勒二阶展开，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mtjds7qj309r0193yg.jpg)

# xgboost如何使用MAE或MAPE作为目标函数？
MAE:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mxhhvg8j303l011q2q.jpg)
MAPE:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mx7uyuej303f0170sj.jpg)
- 利用可导的函数逼近MAE或MAPE
    - mse
    - Huber loss
    - Pseudo-Huber loss

# xgboost如何寻找分裂节点的候选集？
- 暴力枚举
    - 法尝试所有特征和所有分裂位置，从而求得最优分裂点。当样本太大且特征为连续值时，这种暴力做法的计算量太大
- 近似算法（approx）
    - 近似算法寻找最优分裂点时不会枚举所有的特征值，而是对特征值进行聚合统计，然后形成若干个桶。然后仅仅将桶边界上的特征的值作为分裂点的候选，从而获取计算性能的提升
        - 离散值直接分桶
        - 连续值分位数分桶

# xgboost如何处理缺失值？
- 训练时：缺失值数据会被分到左子树和右子树分别计算损失，选择较优的那一个
- 预测时：如果训练中没有数据缺失，预测时出现了数据缺失，那么默认被分类到右子树

# xgboost在计算速度上有了哪些点上提升？
- 特征预排序
    - 按特征进行存储，每一个block代表一个特征的值，样本在该block中按照它在该特征的值排好序。这些block只需要在程序开始的时候计算一次，后续排序只需要线性扫描这些block即可
    - block可以仅存放样本的索引，而不是样本本身，这样节省了大量的存储空间

# xgboost特征重要性是如何得到的？
- ’weight‘：代表着某个特征被选作分裂结点的次数；
- ’gain‘：使用该特征作为分类结点的信息增益；
- ’cover‘：某特征作为划分结点，覆盖样本总数的平均值；

# XGBoost中如何对树进行剪枝？
- 在目标函数中增加了正则项：叶子结点树+叶子结点权重的L2模的平方
- 在结点分裂时，定义了一个阈值，如果分裂后目标函数的增益小于该阈值，则不分裂
- 当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值（最小样本权重和），也会放弃此次分裂
- XGBoost 先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的结点，进行剪枝

# XGBoost模型如果过拟合了怎么解决？
- 直接修改模型：
    - 降低树的深度
    - 增大叶子结点的权重
    - 增大惩罚系数
- subsample的力度变大，降低异常点的影响
- 减小learning rate，提高estimator

# xgboost如何调参数？
- 先确定learningrate和estimator
- 再确定每棵树的基本信息，max_depth和 min_child_weight
- 再确定全局信息：比如最小分裂增益，子采样参数，正则参数
- 重新降低learningrate，得到最优解


学习笔记: 2020-4-18
    我是初次学习算法与数据结构，之前也不是科班出身，在第一周初次接触算法与数据结构的学习的过程当遇到了不少的问题，我就以初学者在理解概念和做题时遇到问题如何解决这一角度来谈谈自己的感受。
    首先，在做题方面，因为刚接触写代码，之前一直认为编程写代码就是打一堆英文字母的缩写就完事， 但是真正接触了代码之后才发现其中的逻辑结构之严密，需要考虑到方方面面， 比如在做“盛最多水”这个题的时候，需要考虑到所有的假设空间，我一开始只是想到for循环来遍历每一根柱子所围成的面积，一开始的测试样例通过了，沾沾自喜地提交(submit)之后发现没有通过， 想了好久也不知道问题出现在哪，后来想到覃老师的话，我忽然意识到自己又陷入了之前做数学题的坏习惯“死磕”，然后看了题解之后才弄明白，使用while循环可以完整地遍历整个条形的情况。当然，小白学算法所遇到的困难不仅仅是做题没思路，就是在概念理解上也会觉得不知所云，这就是我接下来要讲的第二点。
    我在看视频课的时候发现并不是想象中零基础教授算法和数据结构的概念，老师讲的结构和逻辑都很清楚，但是做为一个一点基础也没有的我来说听第一遍的时候总是云里雾里，处于懵的状态，当然，我在这一点也做了应对措施，我讲讲我的应对经验。我先用1.25倍（老师建议1.5倍，我真的跟不上，手动狗头）的速度快速听一遍，尽可能地把自己听懂的总结出来，然后立马再看一遍，把老师讲的每一个要点都总结在本上，然后与第一遍总结的做对比，完善，第二天早上再看一遍笔记，理解，然后再看老师讲的内容就很有体会了。但是还是有很多基本知识点老师可能觉得太基本的没讲，然后从书中找到相应的章节看一遍，这样就能很好的理解本周所讲的内容了。
    总之，对于小白来学这个课的基本就是几个字，“用心，重复”，正如覃老师所讲的“过遍数”。刚开始学这个领域的只是，感觉到新奇，有意思，我想我能在这“70天”的刻意练习中收获多多，加油！

//查询和使用stack、queue和deque的接口
//stack和queue的背后是用一个数组来进行模拟， 在这个数组上加一些API来实现stack或者一个queue
1、使用列表作为栈stack
使用list方法可以讲列表作为栈，其中最后添加的元素是检索到的第一个元素（"先进后厨"）。要将项目添加到栈的顶部，使用append()。要从栈顶部检索元素，使用pop()
>>>stack = [3, 4, 5]
>>>stack.append(6)
stack = [3, 4, 5, 6]
>>>stack.pop()
6
stack = [3, 4, 5]
2、使用列表作为队列
也可以将列表用作队列，其中添加的第一个元素是检索到的第一个元素（“先进先出”）
>>>from collections import deque
>>>queue = deque(["a", "b", "c"])
>>>queue.append("d")
>>>queue.popleft()